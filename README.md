# LLM
A Tutorial Implementation and Documentation of LLM

Our brains hold human intelligence.

Our brains are consisted of neurons that are connected.

If we use N to represent a neuron, then the simplest representation of brain is:
            N*

Inside each neuron are dendrites that receive signals, axons that send out signals, and the cell body in between.

Thus a more detailed model of our brain is:

        [i*-S-o*]*

Where [i*-S-o*] is a neuron, and i* are the Input dentrites, o* are the Output axons, and S is the body of neuron.

Our brains are born innocent and almost identical. Structurally completed, they can do almost nothing, except for their ability to learn. It is the structure per se, but the Learning ability that give rise to all human intelligence. 

The most successful Large Language Models(LLM) are organized similarly as the neural networks of our brains. Once structurally created, the LLM behaves the same as a new born brain, knowing nothing. Nevertheless, this software simulation of our brain exibits extraordinary learning ability.

The key for the success of chatgpt LLM is the additional contextual algorithm that is somewhat similar to correlation matrix among tokens.

The chatgpt neural network consists of three underlying algorithms:
  1. Neural Networks
  2. Back propagation
  3. Contex processing <--New chatgpt addition.

We therefore make model implementation of each of these three algorithms.
